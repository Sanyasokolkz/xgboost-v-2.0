"""
Solana Memtoken Predictor API - –ü–æ–ª–Ω–∞—è –≤–µ—Ä—Å–∏—è
–†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –Ω–∞ Railway.app —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
"""

import os
import joblib
import pickle
import numpy as np
import pandas as pd
from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
import logging
from datetime import datetime
import traceback
import re

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –°–æ–∑–¥–∞–µ–º Flask –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
app = Flask(__name__)
CORS(app)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏
model_artifacts = None
MODEL_VERSION = "1.0.0"

def parse_value(val):
    """–ü–∞—Ä—Å–∏–Ω–≥ –∑–Ω–∞—á–µ–Ω–∏–π —Å —Å—É—Ñ—Ñ–∏–∫—Å–∞–º–∏ K, M, B"""
    if pd.isna(val) or val == '' or val is None:
        return np.nan
    if isinstance(val, (int, float)):
        return float(val)
    
    val = str(val).replace('$', '').replace(',', '').strip()
    multipliers = {'K': 1e3, 'M': 1e6, 'B': 1e9, 'T': 1e12}
    
    for suffix, mult in multipliers.items():
        if val.upper().endswith(suffix):
            try:
                return float(val[:-1]) * mult
            except:
                return np.nan
    
    try:
        return float(val)
    except:
        return np.nan

def parse_time(time_str):
    """–ü–∞—Ä—Å–∏–Ω–≥ –≤—Ä–µ–º–µ–Ω–∏ –≤ –º–∏–Ω—É—Ç—ã"""
    if pd.isna(time_str) or time_str is None:
        return np.nan
    
    total_minutes = 0
    parts = str(time_str).split()
    
    for part in parts:
        try:
            if 'd' in part:
                total_minutes += int(part.replace('d', '')) * 1440
            elif 'h' in part:
                total_minutes += int(part.replace('h', '')) * 60
            elif 'm' in part:
                total_minutes += int(part.replace('m', ''))
        except:
            continue
    
    return total_minutes if total_minutes > 0 else np.nan

def parse_alpha_one_text(text):
    """
    –ü–∞—Ä—Å–∏—Ç —Ç–µ–∫—Å—Ç —Ñ–æ—Ä–º–∞—Ç–∞ alpha_one –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    """
    if not text or not isinstance(text, str):
        return {}
    
    data = {}
    
    try:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–∫–µ–Ω —Å–∏–º–≤–æ–ª –∏ –Ω–∞–∑–≤–∞–Ω–∏–µ
        token_match = re.search(r'\$(\w+)\s*\|\s*(\w+)', text)
        if token_match:
            data['token_symbol'] = token_match.group(1)
            data['token_name'] = token_match.group(2)
        
        # –ê–¥—Ä–µ—Å —Ç–æ–∫–µ–Ω–∞
        address_match = re.search(r'([A-Za-z0-9]{40,50})', text)
        if address_match:
            data['token_address'] = address_match.group(1)
        
        # –í–æ–∑—Ä–∞—Å—Ç —Ç–æ–∫–µ–Ω–∞
        age_match = re.search(r'Token Age:\s*([0-9]+[mhd]\s*[0-9]*[ms]*)', text)
        if age_match:
            data['token_age'] = age_match.group(1).strip()
        
        # Market Cap
        mcap_match = re.search(r'MCap:\s*\$([0-9.]+[KMB]?)', text)
        if mcap_match:
            data['market_cap'] = mcap_match.group(1)
        
        # Liquidity
        liq_match = re.search(r'Liq:\s*\$([0-9.]+[KMB]?)', text)
        if liq_match:
            data['liquidity'] = liq_match.group(1)
        
        # Volume 5min (–∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ volume_1min)
        vol_match = re.search(r'Volume 5min:\s*\$([0-9.]+[KMB]?)', text)
        if vol_match:
            data['volume_1min'] = vol_match.group(1)
        
        # Last Volume –∏ –º–Ω–æ–∂–∏—Ç–µ–ª—å
        last_vol_match = re.search(r'Last Volume:\s*\$([0-9.]+[KMB]?)\s*([0-9.]+x)', text)
        if last_vol_match:
            data['last_volume'] = last_vol_match.group(1)
            data['last_volume_multiplier'] = last_vol_match.group(2)
        
        # –ü–∞—Ä—Å–∏–º –¥–µ—Ä–∂–∞—Ç–µ–ª–µ–π –ø–æ —ç–º–æ–¥–∑–∏
        emoji_patterns = {
            'green_holders': r'üü¢:\s*([0-9]+)',
            'blue_holders': r'üîµ:\s*([0-9]+)', 
            'yellow_holders': r'üü°:\s*([0-9]+)',
            'circle_holders': r'‚≠ïÔ∏è:\s*([0-9]+)',
            'clown_holders': r'ü§°:\s*([0-9]+)',
            'sun_holders': r'üåû:\s*([0-9]+)',
            'half_moon_holders': r'üåó:\s*([0-9]+)',
            'dark_moon_holders': r'üåö:\s*([0-9]+)'
        }
        
        for key, pattern in emoji_patterns.items():
            match = re.search(pattern, text)
            if match:
                data[key] = int(match.group(1))
        
        # Total –∏ Total now –ø—Ä–æ—Ü–µ–Ω—Ç—ã
        total_match = re.search(r'Total:\s*([0-9.]+)%%', text)
        if total_match:
            data['total_percent'] = float(total_match.group(1))
        
        total_now_match = re.search(r'Total now:\s*([0-9.]+)%%', text)
        if total_now_match:
            data['total_now_percent'] = float(total_now_match.group(1))
        
        # Top10 –ø—Ä–æ—Ü–µ–Ω—Ç
        top10_match = re.search(r'Top10:\s*([0-9.]+)%', text)
        if top10_match:
            data['top10_percent'] = float(top10_match.group(1))
        
        # Total holders - –∏—â–µ–º –ø–æ—Å–ª–µ "Total:" –Ω–æ –ù–ï –ø–æ—Å–ª–µ "Total:" —Å –ø—Ä–æ—Ü–µ–Ω—Ç–∞–º–∏
        holders_match = re.search(r'Total:\s*([0-9]+)(?:\s|$)', text)
        if holders_match:
            data['total_holders'] = int(holders_match.group(1))
        
        # Insiders
        insiders_match = re.search(r'Insiders:\s*([0-9]+)\s*hold\s*([0-9.]+)%', text)
        if insiders_match:
            data['insiders_count'] = int(insiders_match.group(1))
            data['insiders_percent'] = float(insiders_match.group(2))
        else:
            # –ï—Å–ª–∏ –Ω–µ—Ç hold, –ø–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ—Ü–µ–Ω—Ç—ã
            insiders_pct_match = re.search(r'Insiders:.*?([0-9.]+)%', text)
            if insiders_pct_match:
                data['insiders_percent'] = float(insiders_pct_match.group(1))
                data['insiders_count'] = 0
        
        # Snipers
        snipers_match = re.search(r'Snipers:\s*([0-9]+)', text)
        if snipers_match:
            data['snipers_count'] = int(snipers_match.group(1))
        
        # Bundle Total
        bundle_total_match = re.search(r'Bundle:.*?Total:\s*([0-9]+)', text, re.DOTALL)
        if bundle_total_match:
            data['bundle_total'] = int(bundle_total_match.group(1))
        
        # Bundle Supply
        bundle_supply_match = re.search(r'Supply:\s*([0-9.]+)%', text)
        if bundle_supply_match:
            data['bundle_supply_percent'] = float(bundle_supply_match.group(1))
        
        # Dev holds
        dev_holds_match = re.search(r'Dev holds\s*([0-9.]+)%', text)
        if dev_holds_match:
            data['dev_holds_percent'] = float(dev_holds_match.group(1))
        
        logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ –ø–æ–ª–µ–π –∏–∑ —Ç–µ–∫—Å—Ç–∞: {len(data)}")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–∞: {str(e)}")
    
    return data

def apply_feature_engineering(df, model_artifacts):
    """–ü—Ä–∏–º–µ–Ω—è–µ—Ç feature engineering –∫ –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º"""
    df = df.copy()
    
    try:
        # –ü–∞—Ä—Å–∏–Ω–≥ –±–∞–∑–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        for col in ['market_cap', 'liquidity', 'volume_1min', 'last_volume']:
            if col in df.columns:
                df[f'{col}_numeric'] = df[col].apply(parse_value)
        
        if 'last_volume_multiplier' in df.columns:
            df['volume_multiplier'] = pd.to_numeric(
                df['last_volume_multiplier'].astype(str).str.replace('x', ''), 
                errors='coerce'
            )
        
        if 'token_age' in df.columns:
            df['token_age_minutes'] = df['token_age'].apply(parse_time)
        
        # –°–æ–∑–¥–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–±–∞–∑–æ–≤—ã–µ)
        if 'liquidity_numeric' in df.columns and 'market_cap_numeric' in df.columns:
            df['liquidity_ratio'] = df['liquidity_numeric'] / (df['market_cap_numeric'] + 1)
            df['liquidity_ratio_log'] = np.log1p(df['liquidity_ratio'])
        
        if 'volume_1min_numeric' in df.columns and 'liquidity_numeric' in df.columns:
            df['volume_liquidity_ratio'] = df['volume_1min_numeric'] / (df['liquidity_numeric'] + 1)
            df['volume_activity'] = np.log1p(df['volume_liquidity_ratio'])
        
        if 'total_holders' in df.columns and 'market_cap_numeric' in df.columns:
            df['holders_per_mcap'] = df['total_holders'] / (df['market_cap_numeric'] / 1000 + 1)
            df['holder_density'] = np.log1p(df['holders_per_mcap'])
        
        # –ê–Ω–∞–ª–∏–∑ –¥–µ—Ä–∂–∞—Ç–µ–ª–µ–π
        holder_cols = ['green_holders', 'blue_holders', 'yellow_holders', 'circle_holders']
        available_holders = [col for col in holder_cols if col in df.columns]
        
        if len(available_holders) >= 2:
            df['total_active_holders'] = df[available_holders].sum(axis=1)
            df['holder_diversity'] = df[available_holders].std(axis=1) / (df[available_holders].mean(axis=1) + 1)
            
            if 'green_holders' in df.columns and 'blue_holders' in df.columns and 'total_holders' in df.columns:
                df['good_holders_pct'] = (df['green_holders'] + df['blue_holders']) / (df['total_holders'] + 1) * 100
        
        # –†–∏—Å–∫-—Å–∫–æ—Ä–∏–Ω–≥
        risk_cols = ['insiders_percent', 'dev_holds_percent', 'bundle_supply_percent']
        available_risk = [col for col in risk_cols if col in df.columns]
        if available_risk:
            df['total_risk_score'] = df[available_risk].fillna(0).sum(axis=1)
            df['max_risk_score'] = df[available_risk].fillna(0).max(axis=1)
        
        # –°–Ω–∞–π–ø–µ—Ä-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        if 'snipers_count' in df.columns and 'total_holders' in df.columns:
            df['sniper_ratio'] = df['snipers_count'] / (df['total_holders'] + 1)
            df['sniper_density'] = np.log1p(df['sniper_ratio'] * 100)
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        if 'token_age_minutes' in df.columns:
            df['log_age'] = np.log1p(df['token_age_minutes'])
            
            if 'volume_1min_numeric' in df.columns:
                df['volume_per_age'] = df['volume_1min_numeric'] / (df['token_age_minutes'] + 1)
        
        # Momentum
        if 'volume_multiplier' in df.columns:
            df['momentum_score'] = np.log1p(df['volume_multiplier'])
            if 'liquidity_ratio' in df.columns:
                df['momentum_liquidity'] = df['momentum_score'] * df['liquidity_ratio']
        
        # –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
        if 'log_age' in df.columns and 'total_risk_score' in df.columns:
            df['age_risk_interaction'] = df['log_age'] * df['total_risk_score']
        
        if 'liquidity_ratio' in df.columns and 'volume_activity' in df.columns:
            df['liquidity_activity'] = df['liquidity_ratio'] * df['volume_activity']
        
        if 'total_holders' in df.columns and 'top10_percent' in df.columns:
            df['holders_concentration'] = df['total_holders'] * (100 - df['top10_percent']) / 100
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–∏–∑ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏)
        # –ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
        if 'liquidity_ratio' in df.columns:
            df['liquidity_ratio_squared'] = df['liquidity_ratio'] ** 2
            df['liquidity_ratio_sqrt'] = np.sqrt(df['liquidity_ratio'].clip(0))
        
        # –ë–∏–Ω–Ω–∏–Ω–≥
        if 'total_holders' in df.columns:
            df['holders_bins'] = pd.cut(df['total_holders'], 
                                       bins=[0, 50, 150, 300, 1000, float('inf')], 
                                       labels=[0, 1, 2, 3, 4]).astype(float)
        
        if 'top10_percent' in df.columns:
            df['concentration_level'] = pd.cut(df['top10_percent'], 
                                              bins=[0, 20, 40, 60, 80, 100], 
                                              labels=[0, 1, 2, 3, 4]).astype(float)
        
        # –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
        if 'insiders_percent' in df.columns and 'dev_holds_percent' in df.columns:
            df['insider_dev_interaction'] = df['insiders_percent'] * df['dev_holds_percent']
        
        if 'total_risk_score' in df.columns and 'top10_percent' in df.columns:
            df['risk_concentration'] = df['total_risk_score'] * df['top10_percent'] / 100
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        if 'token_age_minutes' in df.columns:
            df['is_fresh'] = (df['token_age_minutes'] < 30).astype(int)
            df['is_golden_hour'] = ((df['token_age_minutes'] >= 30) & 
                                   (df['token_age_minutes'] < 120)).astype(int)
            df['is_mature'] = (df['token_age_minutes'] >= 120).astype(int)
        
        # –ö–∞—á–µ—Å—Ç–≤–æ –¥–µ—Ä–∂–∞—Ç–µ–ª–µ–π
        if len(available_holders) >= 2 and 'total_holders' in df.columns:
            df['quality_holder_ratio'] = df[available_holders].sum(axis=1) / (df['total_holders'] + 1)
            df['holder_quality_score'] = (df.get('green_holders', 0) * 3 + 
                                         df.get('blue_holders', 0) * 2 + 
                                         df.get('yellow_holders', 0) * 1) / (df['total_holders'] + 1)
        
        # –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
        if all(col in df.columns for col in ['volume_1min_numeric', 'liquidity_numeric', 'market_cap_numeric']):
            df['volume_mcap_ratio'] = df['volume_1min_numeric'] / (df['market_cap_numeric'] + 1)
            df['liquidity_efficiency'] = df['volume_1min_numeric'] / (df['liquidity_numeric'] + 1)
            df['activity_composite'] = (np.log1p(df['volume_mcap_ratio']) + 
                                       np.log1p(df['liquidity_efficiency'])) / 2
        
        # –ú–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –∏ —Å–Ω–∞–π–ø–µ—Ä—ã
        if all(col in df.columns for col in ['snipers_count', 'total_holders', 'insiders_percent']):
            df['sniper_insider_combo'] = (df['snipers_count'] / (df['total_holders'] + 1)) * df['insiders_percent']
            df['manipulation_risk'] = (df.get('sniper_density', 0) + df['insiders_percent']) / 2
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        if 'token_age_minutes' in df.columns and model_artifacts.get('label_encoder'):
            age_category = pd.cut(df['token_age_minutes'], 
                                 bins=[0, 15, 30, 60, 180, 1440, float('inf')],
                                 labels=['fresh', 'new', 'young', 'mature', 'old', 'ancient'])
            try:
                df['age_category_encoded'] = model_artifacts['label_encoder'].transform(age_category.fillna('unknown'))
            except:
                df['age_category_encoded'] = 0  # Fallback
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ feature engineering: {str(e)}")
        # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å —Ç–µ–º, —á—Ç–æ –ø–æ–ª—É—á–∏–ª–æ—Å—å
    
    return df

def load_model():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    global model_artifacts
    
    try:
        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏
        if os.path.exists('solana_memtoken_model.pkl'):
            model_artifacts = joblib.load('solana_memtoken_model.pkl')
            logger.info("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ solana_memtoken_model.pkl")
        elif os.path.exists('solana_memtoken_model_pickle.pkl'):
            with open('solana_memtoken_model_pickle.pkl', 'rb') as f:
                model_artifacts = pickle.load(f)
            logger.info("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ solana_memtoken_model_pickle.pkl")
        else:
            raise FileNotFoundError("–§–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
        required_keys = ['model', 'imputer', 'scaler', 'feature_names']
        for key in required_keys:
            if key not in model_artifacts:
                raise KeyError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–ª—é—á: {key}")
        
        logger.info(f"–ú–æ–¥–µ–ª—å —Å–æ–¥–µ—Ä–∂–∏—Ç {len(model_artifacts['feature_names'])} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        logger.info(f"–¢–∏–ø –º–æ–¥–µ–ª–∏: {model_artifacts.get('model_type', 'Unknown')}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {str(e)}")
        return False

def predict_token_success(token_data):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
    try:
        if model_artifacts is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —ç—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ alpha_one –∏–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ
        if isinstance(token_data, dict) and len(token_data) == 1:
            # –í–æ–∑–º–æ–∂–Ω–æ —ç—Ç–æ alpha_one —Ñ–æ—Ä–º–∞—Ç
            first_key = list(token_data.keys())[0]
            first_value = token_data[first_key]
            
            if isinstance(first_value, str) and len(first_value) > 100:
                # –≠—Ç–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ alpha_one —Ç–µ–∫—Å—Ç
                logger.info("–û–±–Ω–∞—Ä—É–∂–µ–Ω alpha_one —Ñ–æ—Ä–º–∞—Ç, –ø–∞—Ä—Å–∏–º —Ç–µ–∫—Å—Ç...")
                parsed_data = parse_alpha_one_text(first_value)
                if parsed_data:
                    token_data = parsed_data
                    logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(parsed_data)} –ø–æ–ª–µ–π –∏–∑ —Ç–µ–∫—Å—Ç–∞")
                else:
                    logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å alpha_one —Ç–µ–∫—Å—Ç")
        
        # –°–æ–∑–¥–∞–µ–º DataFrame –∏–∑ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        df_input = pd.DataFrame([token_data])
        logger.info(f"–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {list(df_input.columns)}")
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º feature engineering
        df_processed = apply_feature_engineering(df_input, model_artifacts)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        for feature in model_artifacts['feature_names']:
            if feature not in df_processed.columns:
                df_processed[feature] = 0.0
        
        # –û—Ç–±–∏—Ä–∞–µ–º –Ω—É–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
        X_input = df_processed[model_artifacts['feature_names']]
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º preprocessing
        X_imputed = model_artifacts['imputer'].transform(X_input)
        X_scaled = model_artifacts['scaler'].transform(X_imputed)
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        probability = float(model_artifacts['model'].predict_proba(X_scaled)[0, 1])
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä–æ–≥
        threshold = model_artifacts.get('best_threshold', 0.5)
        prediction = int(probability >= threshold)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        confidence_score = abs(probability - 0.5) * 2
        if confidence_score > 0.6:
            confidence = '–í—ã—Å–æ–∫–∞—è'
        elif confidence_score > 0.3:
            confidence = '–°—Ä–µ–¥–Ω—è—è'
        else:
            confidence = '–ù–∏–∑–∫–∞—è'
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é
        if probability >= 0.7:
            recommendation = '–ü–û–ö–£–ü–ê–¢–¨'
            recommendation_color = 'success'
        elif probability >= 0.4:
            recommendation = '–û–°–¢–û–†–û–ñ–ù–û'
            recommendation_color = 'warning'
        else:
            recommendation = '–ò–ó–ë–ï–ì–ê–¢–¨'
            recommendation_color = 'danger'
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            'success': True,
            'prediction': '–£–°–ü–ï–•' if prediction == 1 else '–ù–ï–£–î–ê–ß–ê',
            'probability': round(probability, 4),
            'probability_percent': f"{probability*100:.1f}%",
            'confidence': confidence,
            'confidence_score': round(confidence_score, 3),
            'recommendation': recommendation,
            'recommendation_color': recommendation_color,
            'threshold_used': threshold,
            'parsed_fields': len(token_data) if isinstance(token_data, dict) else 0,
            'model_info': {
                'type': model_artifacts.get('model_type', 'Unknown'),
                'version': MODEL_VERSION,
                'features_count': len(model_artifacts['feature_names'])
            }
        }
        
        return result
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {str(e)}")
        logger.error(traceback.format_exc())
        return {
            'success': False,
            'error': str(e),
            'prediction': '–û–®–ò–ë–ö–ê',
            'probability': 0.0,
            'probability_percent': '0.0%'
        }

# =============================================================================
# ROUTES (–ú–∞—Ä—à—Ä—É—Ç—ã API)
# =============================================================================

@app.route('/')
def home():
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞"""
    return render_template('index.html')

@app.route('/health')
def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    return jsonify({
        'status': 'healthy',
        'model_loaded': model_artifacts is not None,
        'version': MODEL_VERSION,
        'timestamp': datetime.now().isoformat()
    })

@app.route('/api/predict', methods=['POST'])
def api_predict():
    """API —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        if request.is_json:
            token_data = request.get_json()
        else:
            token_data = request.form.to_dict()
        
        if not token_data:
            return jsonify({
                'success': False,
                'error': '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞'
            }), 400
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å (–±–µ–∑ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)
        logger.info(f"–ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {len(token_data)} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        
        # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        result = predict_token_success(token_data)
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ API: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞: {str(e)}'
        }), 500

@app.route('/api/batch_predict', methods=['POST'])
def api_batch_predict():
    """–ü–∞–∫–µ—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤"""
    try:
        data = request.get_json()
        
        if not data or 'tokens' not in data:
            return jsonify({
                'success': False,
                'error': '–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö. –û–∂–∏–¥–∞–µ—Ç—Å—è: {"tokens": [...]}'
            }), 400
        
        tokens = data['tokens']
        if len(tokens) > 100:  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
            return jsonify({
                'success': False,
                'error': '–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤. –ú–∞–∫—Å–∏–º—É–º 100 –∑–∞ —Ä–∞–∑.'
            }), 400
        
        results = []
        for i, token_data in enumerate(tokens):
            try:
                result = predict_token_success(token_data)
                result['token_index'] = i
                results.append(result)
            except Exception as e:
                results.append({
                    'token_index': i,
                    'success': False,
                    'error': str(e)
                })
        
        return jsonify({
            'success': True,
            'results': results,
            'total_processed': len(results)
        })
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ batch API: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/parse_text', methods=['POST'])
def api_parse_text():
    """–≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ alpha_one —Ç–µ–∫—Å—Ç–∞"""
    try:
        data = request.get_json()
        
        if not data or 'text' not in data:
            return jsonify({
                'success': False,
                'error': '–¢—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–ª–µ "text"'
            }), 400
        
        text = data['text']
        parsed_data = parse_alpha_one_text(text)
        
        return jsonify({
            'success': True,
            'parsed_data': parsed_data,
            'fields_extracted': len(parsed_data)
        })
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/model_info')
def model_info():
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"""
    if model_artifacts is None:
        return jsonify({
            'success': False,
            'error': '–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞'
        }), 503
    
    return jsonify({
        'success': True,
        'model_info': {
            'version': MODEL_VERSION,
            'type': model_artifacts.get('model_type', 'Unknown'),
            'features_count': len(model_artifacts['feature_names']),
            'feature_names': model_artifacts['feature_names'][:20],  # –ü–µ—Ä–≤—ã–µ 20
            'performance_metrics': model_artifacts.get('performance_metrics', {}),
            'threshold': model_artifacts.get('best_threshold', 0.5),
            'training_info': model_artifacts.get('training_info', {})
        }
    })

@app.errorhandler(404)
def not_found(error):
    return jsonify({
        'success': False,
        'error': '–≠–Ω–¥–ø–æ–∏–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω'
    }), 404

@app.errorhandler(500)
def internal_error(error):
    return jsonify({
        'success': False,
        'error': '–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞'
    }), 500

# =============================================================================
# –ó–ê–ü–£–°–ö –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø
# =============================================================================

if __name__ == '__main__':
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ Solana Memtoken Predictor...")
    
    if not load_model():
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å. –í—ã—Ö–æ–¥.")
        exit(1)
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Ä—Ç –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è (Railway)
    port = int(os.environ.get('PORT', 5000))
    
    logger.info(f"‚úÖ –°–µ—Ä–≤–µ—Ä –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –Ω–∞ –ø–æ—Ä—Ç—É {port}")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º Flask –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
    app.run(
        host='0.0.0.0',
        port=port,
        debug=os.environ.get('FLASK_ENV') == 'development'
    )